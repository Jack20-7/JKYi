1.日志系统和配置系统的整合
对于配置系统，在log.yam作用的统一个格式是
logs:
   -name: root
    level: (debug,error,info...)
    formatter: '%d %T %t %T %F %T [%p] %T %f %T %l %T <%m> %n'
    appender: 
      -type: (StdoutAppender,FileAppender)
       level: (debug,info,error...)
       file: /home/admin/workSpace/log.txt
//以上是默认的格式，用户如果需要的话，可以在下面以上面那种方式进行添加或者修改

//2022.5.27
不要在头文件里面写函数的定义，除非将定义放在类里面或者是哟个inline关键字进行修饰，不然的话如果该头文件被多个源文件包含的话，就会造成该函数被编译多次
从而出现函数重定义的问题。
//2022.5.28
如果想要一个类不能够被拷贝的话，我们可以额外定义一个noncopyable的类型，将该类的默认构造和默认析构定义为default，然后将其他的拷贝构造、拷贝赋值、移动构造、移动赋值全部都声明为delete。然后我们将直接让不能够被拷贝的类继承noncopyalbe就行了
//2022.5.29
封装的互斥器不能够用在const成员函数内部，因为他涉及到修改类中的m_mutex这个成员变量，而const成员函数内部不能够修改没有用mutable修饰的成员变量
经测试，在平凡写的情况下下，自旋锁的效率比互斥锁高3-4倍，原子锁和自旋锁效率相当，但是还是考虑使用自旋锁好一点，因为自旋锁在尝试几次抢不到锁之后就会陷入睡眠，而原子锁会在while中一直尝试获取锁，从而使得CPU利用率降低
//2022.5.30
static成员函数中不能够访问类中非static的成员
static成员函数不能够使用const关键字来进行修饰
在宏定义中，表达式一定要使用()将他给括起来,否则非常容易出错！！！！！！！！！！！！
//2022.6.9
类中的非静态成员变量的初始化顺序应该按照他们的声明顺序在构造函数的成员初始化列表中进行初始化，否则编译期会进行警告甚至报错
//2022.6.16
socket在非阻塞的状态下，当调用connect向服务器请求建立连接时，连接可能不会立刻就建立成功，这个时候connect的返回值是-1并且error为EINPROGRESS。这个时候我们需要对该socket的写事件进行监听，如果发生了写事件并且通过getsocketpot检测到错误码是0时才能够确定连接是建立成功的，否则就是失败的
//2022.6.17
查看当前项目的代码量的shell命令是 find . -name "*.h" -or -name "*.cc" | xargs grep -v "^$" | wc -l
今天简单测试了一下性能，开五个线程将i加到1000000000,1秒不到就完成了，所以可以看出多协程的效率是真的吊
2022.6.18
今天看了一下大端序和小端序转换的问题，系统提供了额对应的宏给我判断当前系统使用的是大端序还是小端序，具体去看endian.h这个文件的定义
如果要我们自己首先大端序-小端序之间的转化的的话，这个就需要借助& 0xff来完成。0xff相当于是1111 1111，然后&运算符表示的是只有只有同为1的时候才会返回1，否则返回的是0
所以&0xff配合在一起可以对二进制数中的任何一部分进行裁剪
比如16位的大端序-小端序之间的转化方式为  ((t&0xff00) >> 8) | ((t&0xff) << 8) 

然后enable_if这个是c++11中的一个新的特性，它的底层实现其实就是一个结构体模板，实现如下:

template<bool Cond, typename T = void> struct enable_if {};

template<typename T> struct enable_if<true, T> { typedef T type; }

并且在C++中，当我们对一个重载函数进行调用的时候，编译期会尝试去推导所以的候选函数以确保得到一个最完美的匹配,并且在推导的过程中涉及到C++语言的一个属性，叫做SFINAE,它的作用就是在推导的过程中如果遇到了那种invalid argument or return type，就会直接将该函数从候选函数中进行删除，编译期不会报错

2022.6.24
C++中成员函数的const属性会影响到重载
判断当前主机字节序的方式就是
  int i=1;
  if((*(char*)&i) == 1){
      //小端序
  }else{
      //大端序
  }

在宏定义里面，如果我们需要查看某一个宏是否存在的话，使用的是defined，如defined.而如果我们要定义宏，使用的是define

2022.7.1日
有符号整形是以补码的形式存储在内存中的，正数的补码就是其二进制本身，而负数的补码是其二进制取反+1后的值，如-1，它对应的补码是1111 1111
无符号整型对于负数，会生成极大的数.无符号整型和有符号整型一起计算时，生成的结果是无符号的
有符号与无符号之间的转化方法具体可以查看bytearray里面的方式
然后bytearray中对于整型采用的压缩方式是protobuf中的varint压缩算法,该算法是用来对无符号整型进行压缩的，压缩的方式是从右往左依次拿到val的7bit的值，然后将该值的最高位设置为1,然后val右移7位，就这样一直到最后一个有效的字节(也就是最高位不等于1的字节).


2022.7.29
服务器框架支持websocket,websocket是一个新的协议，引入的原因是主要为了解决http协议单向的问题.因为对于http协议 1.0/1.1而言，服务器端只能够被动的接受客户端发送来的请求，然后像客户端返回响应，也就是没有请求就没有响应，这样就使得客户端如果想要话获取到服务器的某些资源需要进行轮询，而轮询的开销是比较大的,需要不断的建立TCP连接或者维持着一条TCP连接.websocket就不一样了，websocket支持服务器主动的向客户端返回资源

//网络编程笔记
2022.9.1
在socket地址中，port是使用16位的int来表示的，这也意味着端口号的数量为2 ^ 16个，也就是65536个。而这些端口号中，有一部分是保留端口号，也就是已经被占用的端口号，比如http-80，https-443
ssh-20等，所以如果我们的程序要使用端口号的话，建议使用5000以上的端口号

对于每一个连接状态的TCP socket，在创建的同时也会为其分别创建一个发送缓冲区和接受缓冲区。它们的大小可以进行自定义，然后我们在程序中比如调用write发送数据时，会先将数据从用户缓冲区拷贝到socket的发送缓冲区，然后返回实际发送的字节数。调用write时如果socket的发送缓冲区中有残留的数据或者发送缓冲区中无法容纳要发送的数据时，对于阻塞状态的socket来说会一直挂起，一直直到将所有的数据都放入到socket的发送缓冲区。所以我们可以看出write函数返回时并不代表数据已经被发送到了对端，是代表要发送的数据已经被全部放入到socket的发送缓冲区

2022.9.3
对于建立的TCP连接，在断开时如果想要优雅的断开连接，应该使用shutdown而不是close

TCP的保活机制(keepalive)的引入其实是为了解决长连接所带来的一些隐患的。对于一条TCP连接而言，在断开时需要进行TCP的四次挥手才能够真正的断开。但是比如说客户端突然的死机的话，这样就导致客户端无法向服务器发送FIN报文来告诉服务器连接断开，这样带来得到后果就是服务器可能会一直对这条无效的TCP连接进行维护，使得服务器的资源被浪费掉
为了解决这样问题，TCP进入保活机制。保活机制指的就是TCP连接在建立时会开启一个计时器，如果在指定的内该连接没有进行相关的活动的话，那么保活机制就会被触发，触发之后会通过该TCP连接每个一段时间就向通信方发送探测报文进行探测，如果能够接收到通过方返回的ACK，那么就重启保活计时器。如果连续发送的多个探测报文都没有接收到回应的话，那么就会判定该TCP连接已经死亡.
但是对于TCP提供的这个保活机制的话，在设计的时候由于当时带宽有限，所以一共需要2小时11分15秒(2 + 75 * 9)才能够发现一条死亡连接，这样对于那么时延要求比较敏感的系统来说是无法忍受的，所以对于大部分系统来说，一般都会在应用层实现自己的心跳检测

为了避免糊涂窗口综合征，当接收方的接受缓存中空闲空间时，需要进行判断接收缓冲区中空闲空间的大小是否满足某一个条件，只有在满足的情况下才去让发送方进行发送数据
对于那些频繁发送小数据包的场景，可以通过开启nagel算法来减少网络中数据报的个数,从而避免了由于数据包过多而导致的网络过载的问题
可以通过延迟确认机制来解决由于频繁的进行ACK导致的网络开销
这里要注意的就是nagel和延迟确认机制不能够同时开启，因为这样会导致请求的时延增加

2022.9.4
当我们使用UDPsocket进行编程时，我们也可以在程序中调用connect进行建立连接。对于UCPsocket而言，调用connect时并不会出现像TCP那种建立连接的现象发生，connect要做的仅仅就是建立UDPsocket和目标主机的地址(ip + port)之间的关联。这样做带来的好处就是能够让我们的应用程序接收到异步错误的信息.所以从性能的角度来讲的话，connect能够在一定程序上进行提高

对于一条TCP连接而言，连接在断开时主动断开连接的一方在TCP的第四次挥手之后并不会立刻就断开连接，而是会处于time_wait状态等待2msl之后才能正在的断开，而处于time_wait状态的TCP连接依旧占据着对应的端口,这样就带来一个问题，就是如果是服务器方导致的连接断开的话，那么服务器在连接断开后就无法立刻重新建立连接，而是需要等待2msl才能够重新启动程序,因为端口正被之前那条那条处于time_wait状态的占用着.对于服务器方的程序而言，我们需要允许端口复用，因为当服务器方的程序由于某些异常而崩溃掉之后，一般是需要迅速重启提供服务的，而不能够等带2msl。解决的方式是通过调用setsocketopt这个函数将SO_REUSEADDR这个socket选项的值设置1,这样之后操作系统就允许该socket绑定一个已经存在的端口.TCP中之所以不允许想相同端口号被复用其实是为了避免出现拥有相同四元组的TCP连接，但是由于客户端的port都是随机的，即使服务器段的ip + port都是相同的，出现相同四元组的TCP连接的概念也是极小的，并且其实出现了两条完成相同的TCP连接，也没事。因为当前的linuxOS做了一下优化，每一条TCP连接都拥有对于的时间戳，旧连接的时间戳一定比新连接的时间戳小，通过这种方式也可以对两条TCP连接进行区分
tcp_tw_reuse 和 SO_RESUEADDR之间的区别:
tcp_tw_reuse它属于是内核的选项，我们一般通过他来解决wait_wait过多的危害，当该参数打开之后，系统中那些处于time_wait状态的创建时间超过1s的连接可以直接被复用。该参数一般用在客户端(连接发起方)
SO_REUSEADDR 属于是用户态的选项，通过他可以告诉操作系统内核那些处于time_wait状态的TCP连接的端口号可以复用。这样一般用来连接的服务方，用来保证程序在崩溃后能够在极短的时间内重启
总结:在所有 TCP 服务器程序中，调用 bind 之前请设置 SO_REUSEADDR 套接字选项

2022.9.5
TCP协议也不是完成可靠的，它能够保证数据报不丢失、不重复、不被破环且有序的到达目标主机，但是却不能够保证数据能够被目标主机处理完毕.对于TCP连接的异常情况，我们只能够通过调用read/write来进行感知.异常情况分别发送了Fin的异常情况和未发送Fin报文的异常情况

thread_local 是c++引入的存储修饰符，使用该符号修饰的变量存在的特点:
1.在thread开始时被创建，在thread结束时才会被释放，并且每一个thread都会拥有该变量的一份副本
2.thread_local 只在声明的时候被赋值一次
3.修饰类的成员变量时需要使用statis修饰

2022.9.6
poll技术有一个很好的地方在于对于pollfd数组中那些我们不想进行检测pollfd结构体，我们只需要将它的fd成员设置为-1就行了，这样内核在进行检测时就会将他忽略
然后对于读事件，poll中在注册时可以使用POLLRDNRM,它相比于POLLIN而言，他不包括OOB等带外数据的检测

在socket编程中，当使用write函数来发送数据时，该函数的返回值其实代表的是有多少数据从用户缓冲区被拷贝到socket的发送缓冲区中去.在阻塞IO的情况下，write的返回值是和我们要要送的数据大小相同的，因为在阻塞IO的方式下，write只有在所有的数据都被拷贝到socket的发送缓冲区之后返回返回。而在非阻塞IO的方式下，就只是单纯的返回本次write拷贝到socket发送缓冲区中的数据，不一定等于我们实际要发送的字节数，具体流程如下:
阻塞IO
write发送数据->程序一直阻塞在这里直到数据全部拷贝到socket的发送缓冲区中去->write返回
非阻塞IO
write发送数据->返回->再次调用->再次返回....

2022.9.8
Reactor模型，也就是事件驱动模型,该模式下的程序的基本结构就是在一个时间循环中，通过事件驱动和事件回调的方式来实现业务逻辑.它的好处在于占用的资源少、效率告并且可拓展性强,该模型的核心在于:
1.它存在一个无限循环的事件分发函数，我们也称为reactor线程，该线程要做的工作其实就是通过复用技术来对各自事件进行检测
2.它将所有的IO操作都抽象为事件，每一个事件都通过回调函数来进行处理。我们通常是先为要检测的事件注册好回调函数，然后当事件发送时就会自动的调用回调函数来进行处理

高并发程序的编程模型迭代:
多进程->多线程->多线程+线程池->单reactor单线程->单reactor多线程->多reactor多线程(one loop per thread)->多reactor多线程+线程池
对于我们的网络框架来说，我们应该为每一条TCP连接提供一个应用层面的缓冲区

2022.9.10
linux下提供的异步接口aio系统的函数并不是真正意义上的异步操作，不是OS系统级别支持的，只是单纯的由库函数借助phread来实现的并且只支持磁盘类IO，不支持socket,而windows下提供了一台完整的支持socket的异步编程接口，称为IOCP.在windows下我们可以借助它来实现Proactor编程模型
Proactor模型和Reactor模型一样，都拥有一个无限循环的eventloop线程,但是和reactor不同的是他并不负责处理IO调用，它负责的只是对于read、write操作完成情况下，分发完成事件到不同的处理函数.可以看出proctor模型它感知的是完成事件，在事件完成之后在发起异步读写请求,并且IO操作是由操作系统完成的,而不会向reactor模型一样感知的是就绪事件，每次当事件触发时有自己调用read、write来完成读写操作

2022.9.11
多reactor多线程的网络模型有两点需要注意的:
1.由于使用的是多reactor多线程模型，所以主线程需要创建多个子线程，子线程在创建之后会执行自己的入口函数，但是主线程如何判断子线程已经初始化完毕这是一个问题(多线程间通信)
2.主reactor线程检测到连接请求并且获取到新连接的文件描述符后，如何将他注册到子线程(因为子线程也是eventloop线程，也可能阻塞在自己的io复用系统调用上)

框架在发送数据时会先判断如果当前socket未注册写事件并且用户层的发送buffer中没有数据的话,会直接调用write进行发送，倘若write没有发送完的话，才会将剩下的数据拷贝到用户层的发送buffer中并且进行写事件的注册(相当于将剩下的数据交给框架进行接管),之所以要这样做的目的是为了提高发送数据的效率,因为大部分的场景都可以通过write来进行直接发送，这样就可以避免一次从用户自己创建的缓冲区将socket用户层发送缓冲区的拷贝

框架中拥有两层回调结构，最外层回调是框架提供给应用程序的接口，应用程序在使用框架时，就只需要编写自己的回调函数，然后传入框架里面去，这样在程序执行的过程中，框架会外合适的时机对用户注册的回调函数进行调用.第二层回调是最底层的回调,是设置在channel上的回调函数，它们会在socket被检测到事件之后被调用,也完成了应用程序编写的代码在框架中的代入.

2022.9.18
模板类成员函数的声明和 定义应该放在一起.因为编译器使用.cc文件作为编译单元的，对于类模板，如果我们将它的声明和定义分离的话，那么就将类模板的实现编译为.o文件,但是这个时候该类模板没有进行实例化，也就是不存在实例类.这样当模板类在实例化的时候，编译器找不到模板类的一个特例，于是会让链接器去其他文件去查找，由于模板类的实现文件中也不存在该实例，所以会出现链接错误

模板没有使用之前是不会被编译的，因为当模板没有被实例化之前，对于模板参数的类型是不可知的，也就是没有办法得知模板类的大小，编译器无法为其分配内存.

2002.9.23
头文件互相包含也会导致 " xxx not type " 的问题，即使使用预处理命令也不行
shared_ptr以const reference的方式进行参数参数传递时是不会增加引用计数的

2022.9.24
多线程程序的线程数目只取决于当前机器的CPU总核数，与当前的并发量无关.
EventLoop的数目应该是每千兆比特每条的吞吐量配一个
同一个EventLoop中的TcpConnection没有优先级的区别,这样做是为了防止优先级反转
使用mutable修饰的成员变量不能够是引用类型

2022.9.25
对于使用非阻塞IO的事件驱动模型的程序而言，IO线程(EventLoop线程)不能够进行同步操作，因为同步操作可能会阻塞IO线程，从而使得程序无法相应客户端的请求，同步操作我们可以交给线程池去完成
TCP协议的可靠性只能够保证数据完整有序到达目标主机，但是不能够保证数据被成功处理
protobuf是用来进行序列化和反序列化的,之所以要进行序列化主要就是为了方便数据传输.protobuf相比于json、xml来说，它的序列化和反序列化的效率更高，占用的带宽更少

2022.9.26
本机器上在cmake的时候，需要cmake -DCMAKE_CXX_COMPILER=$(which g++) -DCMAKE_C_COMPILER=$(which gcc) ,并且还需要在目标库的CMakeLists文件中添加add_definitions(-D_GLIBCXX_USE_C99=1)
cmake中可以通过option来定义编译开关，语法为option(名称 “描述" on/off),add_definitions用来定义宏
之所以要限制并发连接数的原因:1.避免服务器过载 2.当文件描述符用完时，如果进行accept会很麻烦,会造成busy-loop

2022.9.28
ox7f 表示的是十六进制的一个字节，0x代表是十六进制，7f = 0111 1111.同理，0x80 = 1000 0000
protobuf程序在链接时需要链接上线程库

2022.10.3
reactor模型保证线程安全的方法:如果某一个类不是线程安全的的话，比如TcpConnection类，为了能够使它线程安全，这里的做法是通过令该类的成员函数只能够在对应TcpConnection对象所在的EventLoop线程调用,通过这种方式来使得可以不需要加锁就能够保证线程安全，如果在非对应的EventLoop线程调用成员函数的话，那么就会将该函数放入到对应的EventLoop中的队列中去，然后将对应的EventLoop唤醒，让他去执行该函数

对调用LogAppender的log进行日志输出时，需要传入输出日志的level，并且每一个appender都有自己的输出级别(默认是debug),只有在要输出的日志级别 >= 该appender日志级别时，才会进行输出
LogFormatter负责对输出的格式进行设置，创建时需要传入要输出的格式对于的string，并且会在构造函数中调用init函数，init函数要做的事情就是根据传输的string，将他转化为对于FormatterItem放入到对应的成员数组里面去

Logger在创建时，他的构造函数中会创建默认的LogFormatter，然后在对logger进行addAppender的时候，会先进行判断如果该appender没有对应的formatter的话，会将当前logger的formatter设置为自己的formatter

对于logger，我们在进行log输出日志时，会判断当前logger的appenders是否为空，如果为空的话，那么实际调用的起始就是高logger绑定的m_root的log，其就是root logger来进行日志输出的.
logger我们在进行setFormatter的时候，不仅会设置自己的fotmatter，对于appenders中的那些hasFormatter = false的appener，也会将该formatter设置为他们的formatter,对于addAppender的时候设置formatter的appender，他们的hasFormatter仍然 == false

当需要使用配置模块的时候，对于那些自定义数据类型，我们需要定义自己的lexicalcast<T from,T to>模板函数的偏特化
在log.cc文件中，为了能够使用配置系统，会提前创建一个configVar<set<LogDefine>>类型的名称为logs的全局变量，他的名称会注册到config里面去，并且设置一个回调，该回调会在configvar变量的setValue中被调用.
日志和配置系统之间的配合的方式是:log.cc文件中先先创建一个名称的logs的configVar<set<LogDefine>>类型的全局变量，他的名称和值会在Config中被注册，并且还会通过全局变量在mian函数开始前被创建的特性，给刚刚创建的配置变量设置一个回调。然后当我们在对配合文件进行加载的时候，会得到一个list容器中会得到一个pair(logs,node}的元素，然后根据名称到config中进行查找，由于之前已经创建过了
，所以可以能够找到，于是这里会将这个node转化为string，再通过configvar的fromString这个成员函数来对全局的configvar的值进行设置,fromString这个成员函数就涉及到调用lexical_cast<string,set<LogDefine>>来将string转化为set<LogDefine>,然后再真正设置值之前，会使用新旧两个值调用之前通过全局变量构造函数注册的回调函数，该回调函数内部要做的工作就是对于新的set<LogDefine>中的那些新加入的/修改的LogDefine，会根据他们的值创建新的Logger，放入到LogManager中，并且将新的值中没有的LogDefine对于的logger进行删除，这里删除其实就是将他的level，这样就无法使用了

在进行日志输出的时候，要输出的日志级别必须 >= 所使用的logger的级别，并且对于每一个appender，要输出的日志level 也应该满足 >= appender的level也才能够输出

//Reactor模型中我们创建的线程的类型一共三种:
1.IO线程: IO线程一般会阻塞在IO复用系统调用上来对我们注册的事件进行检测
2.计算线程: 也就是线程池中的线程，通过阻塞在条件变量上等待任务
3,第三方库所使用的线程

在网络库中，如果我们要进行非阻塞编程的话，那么对于每一条TCP连接，我们都应该给他提供用户层的输入Buffer和输出Buffer
输出Buffer的原因:
当我们通过TCP连接向通信方发送100kb大小的数据的话，如果当前Socket的发送缓冲区只有50kb大小的空闲空间的话，那么在阻塞的状态下，我们的程序会一直阻塞在发送数据的函数上直到将剩下的50kb数据拷贝到socket的发送缓冲区之后才会返回。而我们目前进行的是非阻塞的网络编程，所以会直接返回，返回之后如果没有提供发送缓冲区的时候，我们程序就需要一直轮询直到将剩下的数据拷贝到socket的发送缓冲区之后，才会从发送的函数返回。而我们希望的是程序的控制权能够尽快的返回给eventloop线程。所以我们给每一条TCP连接分配一个发送缓冲区，然后这里我们就可以将未发送完的数据保存到发送缓冲区里面去，然后对该TCP连接进行写事件的注册，然后就可以直接发送。这样就相当于将剩下的数据交给了网络库来管理，当该socket的发送缓冲区中有足够的空闲空间的时候，就会触发写事件，然后由写回调函数将剩下的数据拷贝到socket的发送缓冲区里面去进行发送.
输入Buffer的原因:
由于TCP协议是基于字节流的传输层协议，所以对于发送方而言，如果它在发送数据包的时候，比如连续发送了两个发小为100kb的数据，而接受方在读取数据时，由于当前网络库使用的是条件触发，所以我们需要一次性将数据给读取完毕，避免出现busy loop的情况，但是加入第一次只读到了60kb数据的话，那么读到的数据是不完整的，不能够直接进行进行处理，所以可以先将它们保存到输入Buffer中，等待读取到完整的数据包之后，在进行业务逻辑的出路


网络库采用的是条件触发的模式而不是边缘触发，这样做的原因是:
1.为了能够和poll进行兼容，因为epoll它只是在高并发连接但是这些连接大多数是空闲连接的场景下效率高，而在连接数比较少，但是活动连接数比较高的场景它的效率其实和poll是差不都的.
2.条件触发编程更加的容易
3.进行读写操作的时候，不需要等待EAGAIN错误码的出现，可以节省系统调用次数

对于提供给TCP连接的用户层缓冲区的大小，一方面我们希望它的大小尽可能大一些，因为这样每一次读数据的时候，能够读到的数据也就多一些，可以有效减少调用的系统调用的次数。另外一方面，我们希望缓冲区所占用的空间能够尽可能小一些，因为对于每一条TCP连接，都需要提供一个发送缓冲区和接收缓冲区，如果它们的大小是50kb的话，那么10000条并发连接缓冲区所占用的内存空间大小就是1G，并且缓冲区的使用率也是很低的。网络库解决这个问题的方式就是通过readv结合栈空间来解决，每一个缓冲区默认是1032个字节，然后在读取的时候，如果读取的数据量不是很大，缓冲区能够装下的话，那么就直接保存到Buffer中就ok了，而如果数据量比较大，缓冲区装不下的时候，就会将多出来的数据暂时保存到栈空间中去，最后再将栈空间中的数据追加到Buffer里面去

网络库中的TCP连接在关闭的时候，不是直接从通过close来进行关闭的，而是采用优雅的方式来关闭连接的，因为直接通过close来关闭连接所带来的问题就是:假如客户端没有数据要向服务器发送了的时候，它是直接通过close来关闭连接的话，close它不仅会关闭输出流，还会管比较输入流，所以这个时候如果服务器有发送给客户端的数据的话，那么这些数据会被客户端给丢弃掉的,这样就造成了数据的丢失。为了避免出现数据丢失的情况，网络库在关闭TCP连接的时候，采用的是被动关闭的方式，也就是会通过调用shutdowm来关闭TCP连接的输出流，这个时客户端还是会收到FIN报文，于是对于正常的客户端程序而言，就会主动的调用close来关闭连接，然后我们服务器就会在该TCP连接上检测到写事件，然后read在读的时候，会返回0，然后就会通过handleclose回调函数来进行处理，handleclose中就会真正的对这条TCP连接进行关闭，比如会调用用户设置的onConnectionCallback等

介绍一下该项目:
该项目是基于Reactor模式实现的C++网络库，里面实现了定时器功能，Buffer缓存，异步日志，TcpServer等模块，采用的时候在linux系统下进行网络编程最高效的one loop per thread网络编程模型，也就是每一个IO线程中都有一个eventloop，主线程的eventlooop只负责对连接事件进行检测，每当检测到客户端的连接请求并且accpet得到一条TCP连接的时候，都会将它注册到子线程的eventloop中去，之后该TCP连接上所有的读写事件都是由子线程的eventloop来进行处理,通过这种方式，充分的利用了多核CPU的性能

